# Stylebook: Content-Dependent Speaking Style Modeling for Any-to-Any Voice Conversion using Only Speech Data

#### Anonymous submission to Interspeech 2024

 

## Abstract

While many any-to-any voice conversion models succeed in transferring the target speech’s style to the converted speech, they still have limitations in faithfully reproducing the target speaker’s speaking style. In this work, we propose a novel method to extract rich style information from target utterances in an unsupervised manner and to efficiently transfer it to a source speech. Our approach utilizes a self-supervised learning model and a new attention mechanism to collect the speaking styles of a target speaker, each corresponding to the different phonetic content. The collected styles are attended with the source speech’s phonetic content to find the target speaker’s style of pronouncing each content. Then, both the source content and the target style are fed into a diffusion-based decoder to generate the converted speech. Experiment results show that our proposed model can achieve better speaker similarity in any-to-any voice conversion tasks when compared to baseline models.



## Audio Samples

| Source | Target | YourTTS | DiffVC | FreeVC | StylebookVC (proposed) |
| :----: | :----: | :-----: | :----: | :----: | :--------------------: |
|        |        |         |        |        |                        |
|        |        |         |        |        |                        |
|        |        |         |        |        |                        |
|        |        |         |        |        |                        |
|        |        |         |        |        |                        |
|        |        |         |        |        |                        |
|        |        |         |        |        |                        |
|        |        |         |        |        |                        |
|        |        |         |        |        |                        |
|        |        |         |        |        |                        |
|        |        |         |        |        |                        |
|        |        |         |        |        |                        |
|        |        |         |        |        |                        |
|        |        |         |        |        |                        |
|        |        |         |        |        |                        |
|        |        |         |        |        |                        |
|        |        |         |        |        |                        |
|        |        |         |        |        |                        |
|        |        |         |        |        |                        |
|        |        |         |        |        |                        |

